#!/usr/bin/env python3
"""
Compare portage-ng `.merge` outputs against Portage `.emerge` outputs.

Input layout (as generated by your graph tooling):
  /Volumes/Storage/Graph/portage/<category>/<pkg-ver>.merge
  /Volumes/Storage/Graph/portage/<category>/<pkg-ver>.emerge

This script:
  - matches `.merge` ↔ `.emerge` pairs (same basename)
  - extracts the set of packages Portage would merge (N/R/U lines)
  - extracts the set of packages portage-ng would merge (install/update/reinstall actions)
  - compares versions + USE flags (best-effort normalization)
  - compares merge ordering (Kendall tau)
  - aggregates portage-ng Domain assumptions / Blockers sections

Metrics
-------

The comparison produces two families of metrics: set-agreement metrics that
measure *what* packages are selected, and an ordering metric that measures
*in which order* they would be built.

### Set agreement — Jaccard-style metrics (CN, CN+V, CN+V+U)

For each (merge, emerge) pair where Portage succeeded (emerge_ok), define:

    e  = |emerge packages|
    m  = |missing in merge|       (in emerge but not merge)
    x  = |extra in merge|         (in merge but not emerge)
    v  = |version mismatches|     (same cat/name, different version)
    u  = |USE mismatches|         (same cat/name+version, different USE)

The "intersection" and "union" at each granularity are:

    CN level   (ignore version+USE):  I = e − m + v,  U = e + x − v
    CN+V level (ignore USE):          I = e − m,      U = e + x
    CN+V+U level:                     I = e − m − u,  U = e + x + u

Aggregate across all emerge_ok pairs by summing I and U, then:

    CN%     = 100 × ΣI_cn   / ΣU_cn
    CN+V%   = 100 × ΣI_cnv  / ΣU_cnv
    CN+V+U% = 100 × ΣI_cnvu / ΣU_cnvu

These are Jaccard similarity coefficients (|A ∩ B| / |A ∪ B|) applied to
the package sets at each granularity.  A score of 100% means both resolvers
select the exact same packages (at that level of detail).

### Ordering agreement — Kendall tau concordance (Order%)

Even when both resolvers select the same packages, they may schedule them
in a different order.  We measure this using the Kendall tau rank
correlation statistic.

For each emerge_ok pair:

  1. Extract the ordered sequence of cat/name (CN) tuples from each plan,
     keeping only merge actions (install/update/downgrade/reinstall), in the
     order they first appear.
  2. Restrict both sequences to the common CNs (packages present in both).
  3. For every pair of common CNs (a, b), check whether their relative
     order agrees (concordant) or disagrees (discordant / inverted).

        concordant pairs  = #{(a,b) : order_emerge(a)<order_emerge(b)
                                      ⟺ order_merge(a)<order_merge(b)}
        discordant pairs  = total_pairs − concordant_pairs
        total_pairs       = C(|common|, 2) = |common|×(|common|−1)/2

Aggregate across all emerge_ok pairs:

    Order% = 100 × (Σ total_pairs − Σ discordant_pairs) / Σ total_pairs

Order% = 100% means perfect ordering agreement with Portage.  Lower values
indicate the plans would build packages in a different sequence, which
matters for build correctness (a dependency must be built before its
dependents).

Note: not every ordering difference is a bug — multiple valid topological
orderings exist for any DAG.  However, large divergence from Portage's
ordering is a signal that the planner/scheduler may be collapsing build-time
dependency cycles incorrectly.

### Spearman rank correlation (Spearman%)

Spearman's ρ measures rank correlation between the positions of common
packages in the emerge and merge plans.  Unlike Kendall tau which counts
pairwise inversions, Spearman emphasises large rank displacements.  A value
of 100% means identical ordering; 0% means no linear rank relationship.

### Dependency-aware metrics (require --md5-cache)

When the md5-cache directory is provided, the script reads DEPEND and
BDEPEND for each package in the merge plan to build a dependency graph.
This enables two metrics that are not possible with ordering data alone:

**Dep-pair concordance (DepConc%)** — Kendall tau restricted to pairs with
an actual build-dependency edge.  For each pair (A, B) where B is a build
dep of A and both appear in both plans, check whether emerge and merge agree
on their relative order.  Filters out noise from unrelated pairs that
inflates the standard Kendall tau discordance.

**Violation rate (Viol%)** — Self-consistency of the merge plan.  For each
package P in the merge plan, count how many of P's build deps appear *later*
in the plan (i.e. would not yet be built when P is needed).  A violation
rate of 0% means the merge plan perfectly respects its own build
dependencies.

### Install-only cycle breaks

Counts scheduler cycle breaks where every action in the cycle is :install
(no :run actions).  These are potentially problematic because install-order
cycles should ideally be resolved by the prover/planner, not the scheduler.

Usage examples:
  python3 Source/Scripts/compare-merge-emerge.py --root /Volumes/Storage/Graph/portage
  python3 Source/Scripts/compare-merge-emerge.py --root ... --limit 200
  python3 Source/Scripts/compare-merge-emerge.py --root ... --target-regex 'xdg-utils|gobject-introspection'
"""

from __future__ import annotations

import argparse
import concurrent.futures as cf
import dataclasses
import json
import os
import re
import sys
import time
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence, Set, Tuple


ANSI_RE = re.compile(r"\x1b\[[0-9;]*[A-Za-z]")
POWERLINE_RE = re.compile(r"[\ue0b0-\ue0d4]")

# When enabled, include full per-pair lists (missing/extra/mismatches) in output JSON.
# This is useful for CN gap analysis, but makes the report larger.
FULL_LISTS = False
MD5_CACHE_DIR: Optional[Path] = None

# Known genuine tree conflicts where portage-ng domain assumptions are
# correct (Portage only "resolves" them by using stale snapshot data).
# Maps assumed-dep substring → reason.  Domain assumptions whose text
# matches any key are counted separately as tree_conflict_assumptions.
KNOWN_TREE_CONFLICTS = {
    "dev-util/pwntools": "pwntools requires <unicorn-2.1.4, pwndbg requires >=unicorn-2.1.4",
    "dev-util/unicorn":  "pwntools requires <unicorn-2.1.4, pwndbg requires >=unicorn-2.1.4",
}


def strip_ansi(s: str) -> str:
    return POWERLINE_RE.sub("", ANSI_RE.sub("", s))


def split_cat_pnver(cpv: str) -> Tuple[str, str]:
    # cpv: category/pkg-ver
    if "/" not in cpv:
        return ("", cpv)
    c, rest = cpv.split("/", 1)
    return (c, rest)


def split_pn_ver(pnver: str) -> Tuple[str, str]:
    """
    Best-effort split of 'name-version' where version begins with a digit.
    Falls back to whole string as name if no split point found.
    """
    # find last '-' such that remainder starts with digit
    for i in range(len(pnver) - 1, 0, -1):
        if pnver[i - 1] == "-" and i < len(pnver) and pnver[i].isdigit():
            return (pnver[: i - 1], pnver[i:])
    return (pnver, "")


@dataclasses.dataclass(frozen=True)
class PkgKey:
    cat: str
    pn: str
    ver: str
    slot: str = ""

    def cpv(self) -> str:
        base = f"{self.cat}/{self.pn}-{self.ver}" if self.ver else f"{self.cat}/{self.pn}"
        return f"{base}:{self.slot}" if self.slot else base

    def cpv_noslot(self) -> str:
        return f"{self.cat}/{self.pn}-{self.ver}" if self.ver else f"{self.cat}/{self.pn}"


def parse_cpv_token(token: str) -> Optional[PkgKey]:
    """
    Token examples:
      app-text/xmlto-0.0.28-r11::gentoo
      dev-libs/glib-2.84.4-r1:2::gentoo
      x11-misc/xdg-utils-1.2.1-r9
    """
    token = token.strip()
    if not token or "/" not in token:
        return None
    # strip repo suffix
    if "::" in token:
        token = token.split("::", 1)[0]
    # split slot
    slot = ""
    if ":" in token:
        # only split on first ":" after cpv; cpv itself never contains ':'
        base, slot = token.split(":", 1)
    else:
        base = token
    cat, pnver = split_cat_pnver(base)
    pn, ver = split_pn_ver(pnver)
    if not cat or not pn:
        return None
    return PkgKey(cat=cat, pn=pn, ver=ver, slot=slot)


def parse_portage_ng_token(token: str) -> Optional[PkgKey]:
    """
    Portage-ng cpv tokens are of the form:
      category/pkg-version
    where the version may include Gentoo suffixes like _pNNN and -rN.
    Our generic parse_cpv_token/1 already supports that, so this is a small
    semantic alias to keep merge/emerge parsing clearly separated.
    """
    return parse_cpv_token(token)


def normalize_use_token(tok: str) -> Optional[str]:
    raw = tok.strip()
    if not raw:
        return None

    # Portage can append '%' as a display marker.
    # - For global profile use.mask/use.force, it still represents a real USE state:
    #     -sysprof%  -> treat as -sysprof
    # - For some parenthesized tokens like (-verify-sig%), it indicates a flag that is
    #   not part of IUSE for that package (display-only): ignore those.
    if raw.startswith("(") and raw.endswith(")") and "%" in raw:
        return None

    # strip parentheses used by emerge for forced/conditional
    tok = raw.strip("()")
    if not tok:
        return None

    # strip trailing markers
    # emerge output sometimes appends markers like '*' and '%'
    tok = tok.rstrip("*%")
    if not tok:
        return None

    return tok


def parse_use_string(s: str) -> Tuple[Set[str], Set[str]]:
    """
    Return (enabled, disabled) sets from a USE="..." style string.
    Tokens are normalized; unknown forms are ignored.
    """
    enabled: Set[str] = set()
    disabled: Set[str] = set()
    for raw in s.split():
        tok = normalize_use_token(raw)
        if not tok:
            continue
        if tok.startswith("-") and len(tok) > 1:
            disabled.add(tok[1:])
        else:
            enabled.add(tok)
    return enabled, disabled


@dataclasses.dataclass
class EmergePkg:
    key: PkgKey
    action: str  # N/R/U/...
    use_enabled: Set[str]
    use_disabled: Set[str]
    use_known: bool


def detect_emerge_error_kind(txt: str) -> Optional[str]:
    # Keep this coarse; we mainly need "did Portage produce a plan?"
    # Note: Portage can print a partial plan (top-level targets) and then abort
    # due to conflicts, autounmask prompts, or circular dependencies. In that
    # case, we must treat the plan as *not comparable* to portage-ng because
    # the dependency graph is incomplete.
    if "has unmet requirements" in txt and "REQUIRED_USE" in txt:
        return "required_use_unsatisfied"
    if "Error: circular dependencies" in txt or "\n * Error: circular dependencies" in txt:
        return "circular_dependencies"
    # Portage can emit a package list and then abort with a slot conflict:
    #   "Multiple package instances within a single package slot ... slot conflict"
    # In that case there is no valid comparable merge plan.
    if "slot conflict" in txt and "Multiple package instances within a single package slot" in txt:
        return "slot_conflict"
    # Portage can also emit a package list and then abort on blocked packages:
    #   "Conflict: N block ..." + "cannot be installed at the same time"
    # In that case, the plan is not comparable to a successful merge plan.
    if (
        "contains packages which cannot be" in txt
        and "installed at the same time on the same system" in txt
        and ("[blocks B" in txt or ("Conflict:" in txt and " block " in txt))
    ):
        return "blocked_packages"
    if "The following USE changes are necessary to proceed" in txt:
        return "autounmask_required"
    if "backtracking has terminated early" in txt and "autounmask" in txt:
        return "autounmask_backtrack_terminated"
    if "masked by" in txt:
        return "masked"
    if "There are no ebuilds to satisfy" in txt or "there are no ebuilds to satisfy" in txt:
        return "no_ebuild_satisfies"
    if "keyword" in txt and "unavailable" in txt:
        return "keyword_unavailable"
    if "Dependency resolution took" in txt and "!!!" in txt and "unmet requirements" in txt:
        return "resolution_failed"
    if "One or more updates/rebuilds have been skipped due to a dependency conflict" in txt:
        return "skipped_conflict"
    return None


def _cn_key(key: PkgKey) -> Tuple[str, str]:
    """Extract (category, name) from a PkgKey."""
    return (key.cat, key.pn)


def _kendall_tau(order_a: List[Tuple[str, str]], order_b: List[Tuple[str, str]]) -> Tuple[int, int]:
    """
    Count pairwise ordering inversions between two orderings of common elements.
    Returns (inversions, total_pairs).
    """
    common = set(order_a) & set(order_b)
    if len(common) < 2:
        return (0, 0)
    pos_a = {x: i for i, x in enumerate(order_a) if x in common}
    pos_b = {x: i for i, x in enumerate(order_b) if x in common}
    common_list = sorted(common)
    inversions = 0
    pairs = 0
    for i, a in enumerate(common_list):
        for b in common_list[i + 1 :]:
            pairs += 1
            if (pos_a[a] < pos_a[b]) != (pos_b[a] < pos_b[b]):
                inversions += 1
    return (inversions, pairs)


def _spearman_rho(order_a: List[Tuple[str, str]], order_b: List[Tuple[str, str]]) -> float:
    """Spearman rank correlation on common elements between two orderings."""
    common = set(order_a) & set(order_b)
    n = len(common)
    if n < 2:
        return 0.0
    pos_a = {x: i for i, x in enumerate(order_a) if x in common}
    pos_b = {x: i for i, x in enumerate(order_b) if x in common}
    rank_a = {x: r for r, x in enumerate(sorted(common, key=lambda c: pos_a[c]))}
    rank_b = {x: r for r, x in enumerate(sorted(common, key=lambda c: pos_b[c]))}
    d_sq = sum((rank_a[x] - rank_b[x]) ** 2 for x in common)
    return 1.0 - 6.0 * d_sq / (n * (n * n - 1))


_CYCLE_ACTION_RE = re.compile(r'└─(install|run|update|downgrade|reinstall)─>')


def _flush_cycle(actions: Set[str], counters: Dict[str, int]) -> None:
    """Classify a completed cycle break and update counters."""
    if actions and actions <= {"install", "update", "downgrade", "reinstall"}:
        counters["install_only_cycle_breaks"] += 1


_DEP_ATOM_RE = re.compile(
    r'(?:!!?|[><=~]+)?([a-zA-Z0-9_][a-zA-Z0-9_+-]*/[a-zA-Z0-9_][a-zA-Z0-9_+.-]+)'
)


def _extract_dep_cns(dep_string: str) -> Set[Tuple[str, str]]:
    """Extract (cat, name) tuples from a Gentoo dependency string (best-effort)."""
    result: Set[Tuple[str, str]] = set()
    for m in _DEP_ATOM_RE.finditer(dep_string):
        atom = m.group(1)
        cat, pnver = split_cat_pnver(atom)
        pn, _ver = split_pn_ver(pnver)
        if cat and pn:
            result.add((cat, pn))
    return result


_dep_cache: Dict[str, Set[Tuple[str, str]]] = {}


def _load_pkg_build_deps(cat: str, pnver: str) -> Set[Tuple[str, str]]:
    """Load DEPEND + BDEPEND from md5-cache for a package. Returns empty set if unavailable."""
    if MD5_CACHE_DIR is None:
        return set()
    cache_key = f"{cat}/{pnver}"
    if cache_key in _dep_cache:
        return _dep_cache[cache_key]
    cache_file = MD5_CACHE_DIR / cat / pnver
    deps: Set[Tuple[str, str]] = set()
    if cache_file.exists():
        try:
            for line in cache_file.read_text(errors='replace').splitlines():
                if line.startswith('DEPEND=') or line.startswith('BDEPEND='):
                    deps |= _extract_dep_cns(line.split('=', 1)[1])
        except Exception:
            pass
    _dep_cache[cache_key] = deps
    return deps


def parse_emerge(path: Path) -> Tuple[Dict[str, EmergePkg], Dict[str, str]]:
    """
    Parses Portage emerge output file.
    Returns (mapping by cpv key string (cat/pn-ver), metadata).
    """
    txt = strip_ansi(path.read_text(errors="replace"))
    pkgs: Dict[str, EmergePkg] = {}
    # Example:
    # [ebuild   R    ] app-text/xmlto-0.0.28-r11::gentoo  USE="text* -latex" 0 KiB
    line_re = re.compile(r"^\[\s*ebuild\s+([A-Z])[^]]*\]\s+(\S+)(?:\s+.*)?$", re.IGNORECASE)
    use_re = re.compile(r'USE="([^"]*)"')
    for line in txt.splitlines():
        m = line_re.match(line)
        if not m:
            continue
        act = m.group(1).upper()
        token = m.group(2)
        key = parse_cpv_token(token)
        if not key:
            continue
        use_m = use_re.search(line)
        if use_m:
            en, dis = parse_use_string(use_m.group(1))
            use_known = True
        else:
            en, dis = set(), set()
            use_known = False
        pkgs[key.cpv_noslot()] = EmergePkg(key=key, action=act, use_enabled=en, use_disabled=dis, use_known=use_known)
    meta: Dict[str, str] = {}
    err = detect_emerge_error_kind(txt)
    if err:
        meta["error_kind"] = err
    # Comparable only if we found packages *and* Portage did not signal an error.
    meta["ok"] = "true" if (pkgs and not err) else "false"
    return pkgs, meta


@dataclasses.dataclass
class MergePkg:
    key: PkgKey
    actions: Set[str]  # install/update/reinstall/run/...
    use_enabled: Set[str]
    use_disabled: Set[str]
    use_known: bool


def _classify_domain_assumption(buf: List[str], counters: Dict[str, int]) -> None:
    """Classify a single domain assumption as genuine or known tree conflict."""
    text = " ".join(buf)
    if "blocked packages" in text.lower():
        return
    for pattern in KNOWN_TREE_CONFLICTS:
        if pattern in text:
            counters["tree_conflict_assumptions"] += 1
            return
    counters["domain_assumptions"] += 1


def parse_merge(path: Path) -> Tuple[Dict[str, MergePkg], Dict[str, int]]:
    """
    Parses portage-ng merge output.
    Returns:
      - mapping by cpv key string (cat/pn-ver) to MergePkg (slot usually not printed)
      - assumptions counters extracted from sections
    """
    txt = strip_ansi(path.read_text(errors="replace"))
    pkgs: Dict[str, MergePkg] = {}

    # Action lines include:
    #   install    portage://dev-libs/foo-1.2.3
    #   update     portage://app-text/xmlto-0.0.28-r11 (replaces pkg://...)
    #   downgrade  portage://app-text/xmlto-0.0.27-r1  (replaces pkg://...)
    action_re = re.compile(r"\b(install|update|downgrade|reinstall|run|download)\s+(portage|overlay|pkg)://([A-Za-z0-9+_.-]+/[A-Za-z0-9+_.-]+)\b")
    # portage-ng prints a fancy "USE" label, sometimes like: "USE = "..."
    # and it may wrap long USE strings across multiple lines.
    use_start_re = re.compile(r'\bUSE\b[^=]*=\s*"(.*)$')

    pending_use_for: Optional[str] = None
    collecting_use_for: Optional[str] = None
    collecting_use_buf: List[str] = []

    def finish_use_buf() -> None:
        nonlocal collecting_use_for, collecting_use_buf
        if not collecting_use_for or collecting_use_for not in pkgs:
            collecting_use_for = None
            collecting_use_buf = []
            return
        use_s = " ".join(collecting_use_buf).strip()
        if use_s:
            en, dis = parse_use_string(use_s)
            pkgs[collecting_use_for].use_enabled |= en
            pkgs[collecting_use_for].use_disabled |= dis
            pkgs[collecting_use_for].use_known = True
        collecting_use_for = None
        collecting_use_buf = []

    for line in txt.splitlines():
        # If we're in a wrapped USE block, keep collecting until closing quote.
        if collecting_use_for:
            s = line
            # continuation lines often contain prefix columns; keep the tail after the last '│'
            if "│" in s:
                s = s.split("│")[-1]
            s = s.strip()
            if '"' in s:
                before, _q, _after = s.partition('"')
                if before.strip():
                    collecting_use_buf.append(before.strip())
                finish_use_buf()
            else:
                if s:
                    collecting_use_buf.append(s)
            continue

        m = action_re.search(line)
        if m:
            action = m.group(1)
            cpv = m.group(3)  # cat/pn-ver
            key = parse_portage_ng_token(cpv)
            if key:
                k = key.cpv_noslot()
                cur = pkgs.get(k)
                if not cur:
                    pkgs[k] = MergePkg(key=key, actions={action}, use_enabled=set(), use_disabled=set(), use_known=False)
                else:
                    cur.actions.add(action)
                # If the next lines include a conf USE block, associate it.
                pending_use_for = k
            continue

        um = use_start_re.search(line)
        if um and pending_use_for and pending_use_for in pkgs:
            chunk = um.group(1)
            if '"' in chunk:
                before, _q, _after = chunk.partition('"')
                en, dis = parse_use_string(before)
                pkgs[pending_use_for].use_enabled |= en
                pkgs[pending_use_for].use_disabled |= dis
                pkgs[pending_use_for].use_known = True
            else:
                collecting_use_for = pending_use_for
                collecting_use_buf = [chunk.strip()] if chunk.strip() else []
            continue

        # Reset pending when we leave the conf block / hit new step, etc.
        if pending_use_for and line.strip().startswith("└─"):
            pending_use_for = None

    # If file ended mid-USE block, finalize best-effort.
    if collecting_use_for:
        finish_use_buf()

    # Assumptions: very lightweight counters
    counters: Dict[str, int] = {"domain_assumptions": 0, "tree_conflict_assumptions": 0,
                                 "blockers": 0, "cycle_breaks": 0,
                                 "install_only_cycle_breaks": 0}
    in_domain = False
    in_blockers = False
    in_cycles = False
    domain_buf: List[str] = []
    cycle_actions: Set[str] = set()
    for line in txt.splitlines():
        if ">>> Domain assumptions" in line:
            if in_cycles:
                _flush_cycle(cycle_actions, counters)
                cycle_actions = set()
            in_domain, in_blockers, in_cycles = True, False, False
            domain_buf = []
            continue
        if ">>> Blockers" in line:
            if in_cycles:
                _flush_cycle(cycle_actions, counters)
                cycle_actions = set()
            in_domain, in_blockers, in_cycles = False, True, False
            continue
        if ">>> Cycle breaks" in line:
            in_domain, in_blockers, in_cycles = False, False, True
            continue
        if line.startswith(">>> "):
            if in_cycles:
                _flush_cycle(cycle_actions, counters)
                cycle_actions = set()
            in_domain = in_blockers = in_cycles = False
            continue
        if in_domain:
            stripped = line.strip()
            if stripped.startswith("- "):
                if domain_buf:
                    _classify_domain_assumption(domain_buf, counters)
                domain_buf = [stripped]
            elif domain_buf:
                domain_buf.append(stripped)
        if in_blockers and "blocks" in line:
            counters["blockers"] += 1
        if in_cycles:
            stripped = line.strip()
            if stripped.startswith("- Cycle break"):
                _flush_cycle(cycle_actions, counters)
                cycle_actions = set()
                counters["cycle_breaks"] += 1
            else:
                cam = _CYCLE_ACTION_RE.search(line)
                if cam:
                    cycle_actions.add(cam.group(1))
    _flush_cycle(cycle_actions, counters)
    if domain_buf:
        _classify_domain_assumption(domain_buf, counters)

    return pkgs, counters


def compare_pair(merge_path: Path, emerge_path: Path) -> Dict:
    emerge, emerge_meta = parse_emerge(emerge_path)
    merge, ass = parse_merge(merge_path)

    emerge_keys = set(emerge.keys())
    # Only consider actually-merged packages on the portage-ng side.
    merge_merged_keys = {k for (k, p) in merge.items() if p.actions.intersection({"install", "update", "downgrade", "reinstall", "run"})}

    # If Portage didn't produce a plan for this target, don't treat every merge package
    # as an "extra". Record it separately as "emerge_failed".
    emerge_ok = emerge_meta.get("ok") == "true"
    if emerge_ok:
        missing_in_merge = sorted(emerge_keys - merge_merged_keys)
        extra_in_merge = sorted(merge_merged_keys - emerge_keys)
    else:
        missing_in_merge = []
        extra_in_merge = []

    # Version mismatch: match by cat/pn ignoring version
    emerge_by_cpn = defaultdict(list)
    merge_by_cpn = defaultdict(list)
    for _k, p in emerge.items():
        emerge_by_cpn[(p.key.cat, p.key.pn)].append(p)
    for _k, p in merge.items():
        if not p.actions.intersection({"install", "update", "downgrade", "reinstall", "run"}):
            continue
        merge_by_cpn[(p.key.cat, p.key.pn)].append(p)

    version_mismatches = []
    use_mismatches = []
    use_mismatches_common = []
    use_only_in_emerge_flags_total = 0
    use_only_in_merge_flags_total = 0
    use_common_flags_total = 0
    if emerge_ok:
        for cpn, eps in emerge_by_cpn.items():
            mps = merge_by_cpn.get(cpn)
            if not mps:
                continue
            # Compare chosen versions if both sides picked exactly one version
            if len(eps) == 1 and len(mps) == 1:
                e = eps[0]
                m = mps[0]
                if e.key.ver != m.key.ver:
                    version_mismatches.append(
                        {"cpn": f"{cpn[0]}/{cpn[1]}", "emerge": e.key.cpv(), "merge": m.key.cpv(), "emerge_act": e.action, "merge_act": sorted(m.actions)}
                    )
                # Compare USE only when both sides reported it *and* chose the same version.
                # Otherwise, differences like `-test` are frequently just IUSE differences
                # between versions (e.g. ocaml-4.x vs ocaml-5.x), not a USE resolution bug.
                if (e.key.ver == m.key.ver) and e.use_known and m.use_known:
                    if (e.use_enabled != m.use_enabled) or (e.use_disabled != m.use_disabled):
                        use_mismatches.append(
                            {
                                "cpn": f"{cpn[0]}/{cpn[1]}",
                                "emerge": e.key.cpv(),
                                "merge": m.key.cpv(),
                                "emerge_use_plus": sorted(e.use_enabled),
                                "emerge_use_minus": sorted(e.use_disabled),
                                "merge_use_plus": sorted(m.use_enabled),
                                "merge_use_minus": sorted(m.use_disabled),
                            }
                        )

                    # "Common-only" USE mismatch: only compare flags that are explicitly
                    # mentioned on *both* sides. This avoids inflating mismatches when
                    # one side doesn't print a flag at all (unknown/implicit).
                    e_all = e.use_enabled | e.use_disabled
                    m_all = m.use_enabled | m.use_disabled
                    common = e_all & m_all
                    only_e = e_all - m_all
                    only_m = m_all - e_all
                    use_only_in_emerge_flags_total += len(only_e)
                    use_only_in_merge_flags_total += len(only_m)
                    use_common_flags_total += len(common)

                    diff_flags = sorted(
                        f for f in common if ((f in e.use_enabled) != (f in m.use_enabled))
                    )
                    if diff_flags:
                        use_mismatches_common.append(
                            {
                                "cpn": f"{cpn[0]}/{cpn[1]}",
                                "emerge": e.key.cpv(),
                                "merge": m.key.cpv(),
                                "flags": diff_flags,
                            }
                        )

    # Action mismatch (rough): Portage U/D/R should correspond to update/downgrade/reinstall
    action_mismatches = []
    if emerge_ok:
        for cpn, eps in emerge_by_cpn.items():
            mps = merge_by_cpn.get(cpn)
            if not mps:
                continue
            if len(eps) == 1 and len(mps) == 1:
                e = eps[0]
                m = mps[0]
                merge_act = ("downgrade" if "downgrade" in m.actions
                             else "update" if "update" in m.actions
                             else "reinstall" if "reinstall" in m.actions
                             else "install" if "install" in m.actions
                             else None)
                if merge_act:
                    if e.action == "N" and merge_act != "install":
                        action_mismatches.append({"cpn": f"{cpn[0]}/{cpn[1]}", "emerge_act": e.action, "merge_act": merge_act, "merge_actions": sorted(m.actions)})
                    if e.action in ("R", "U", "D") and merge_act == "install":
                        action_mismatches.append({"cpn": f"{cpn[0]}/{cpn[1]}", "emerge_act": e.action, "merge_act": merge_act, "merge_actions": sorted(m.actions)})
                    if e.action == "U" and merge_act == "downgrade":
                        action_mismatches.append({"cpn": f"{cpn[0]}/{cpn[1]}", "emerge_act": e.action, "merge_act": merge_act, "merge_actions": sorted(m.actions)})
                    if e.action == "D" and merge_act == "update":
                        action_mismatches.append({"cpn": f"{cpn[0]}/{cpn[1]}", "emerge_act": e.action, "merge_act": merge_act, "merge_actions": sorted(m.actions)})

    # Ordering comparison (Kendall tau distance on common CNs).
    # Extract ordered CN sequences from the parsed data.
    # Emerge: insertion order of pkgs dict (Python 3.7+ preserves it).
    # Merge: insertion order of merge_by_cpn (merge actions only).
    order_inversions = 0
    order_pairs = 0
    order_common = 0
    spearman_rho = 0.0
    dep_concordant = 0
    dep_discordant = 0
    dep_pairs = 0
    violations = 0
    dep_edges_in_plan = 0
    if emerge_ok:
        emerge_cn_order: List[Tuple[str, str]] = []
        seen_cn: Set[Tuple[str, str]] = set()
        for p in emerge.values():
            cn = _cn_key(p.key)
            if cn not in seen_cn:
                seen_cn.add(cn)
                emerge_cn_order.append(cn)

        merge_cn_order: List[Tuple[str, str]] = []
        seen_cn = set()
        for p in merge.values():
            if not p.actions.intersection({"install", "update", "downgrade", "reinstall", "run"}):
                continue
            cn = _cn_key(p.key)
            if cn not in seen_cn:
                seen_cn.add(cn)
                merge_cn_order.append(cn)

        common_cn = set(emerge_cn_order) & set(merge_cn_order)
        order_common = len(common_cn)
        if order_common >= 2:
            order_inversions, order_pairs = _kendall_tau(emerge_cn_order, merge_cn_order)
            spearman_rho = _spearman_rho(emerge_cn_order, merge_cn_order)

        # Dependency-aware ordering metrics (require md5-cache)
        if MD5_CACHE_DIR:
            merge_cn_set = set(merge_cn_order)
            merge_cn_pos_map = {cn: i for i, cn in enumerate(merge_cn_order)}
            emerge_cn_pos_map = {cn: i for i, cn in enumerate(emerge_cn_order)} if order_common >= 2 else {}

            for _k, p in merge.items():
                if not p.actions.intersection({"install", "update", "downgrade", "reinstall", "run"}):
                    continue
                cn = _cn_key(p.key)
                if cn not in merge_cn_set:
                    continue
                pnver = f"{p.key.pn}-{p.key.ver}" if p.key.ver else p.key.pn
                build_deps = _load_pkg_build_deps(p.key.cat, pnver)
                for dep_cn in build_deps:
                    if dep_cn == cn:
                        continue
                    # Violation rate: merge plan self-consistency
                    if dep_cn in merge_cn_set:
                        dep_edges_in_plan += 1
                        if merge_cn_pos_map[dep_cn] > merge_cn_pos_map[cn]:
                            violations += 1
                    # Dep-pair concordance: emerge vs merge agreement on dep pairs
                    if dep_cn in common_cn and cn in common_cn and order_common >= 2:
                        dep_pairs += 1
                        e_ok = emerge_cn_pos_map[dep_cn] < emerge_cn_pos_map[cn]
                        m_ok = merge_cn_pos_map[dep_cn] < merge_cn_pos_map[cn]
                        if e_ok == m_ok:
                            dep_concordant += 1
                        else:
                            dep_discordant += 1

    return {
        "merge": str(merge_path),
        "emerge": str(emerge_path),
        "emerge_ok": emerge_ok,
        "emerge_error_kind": emerge_meta.get("error_kind", ""),
        "counts": {
            "emerge_pkgs": len(emerge_keys),
            "merge_pkgs": len(merge_merged_keys),
            "missing_in_merge": len(missing_in_merge),
            "extra_in_merge": len(extra_in_merge),
            "version_mismatches": len(version_mismatches),
            "use_mismatches": len(use_mismatches),
            "use_mismatches_common": len(use_mismatches_common),
            "use_only_in_emerge_flags_total": use_only_in_emerge_flags_total,
            "use_only_in_merge_flags_total": use_only_in_merge_flags_total,
            "use_common_flags_total": use_common_flags_total,
            "action_mismatches": len(action_mismatches),
            "order_inversions": order_inversions,
            "order_pairs": order_pairs,
            "order_common_cns": order_common,
            "spearman_rho": round(spearman_rho, 6),
            "dep_concordant": dep_concordant,
            "dep_discordant": dep_discordant,
            "dep_pairs": dep_pairs,
            "violations": violations,
            "dep_edges_in_plan": dep_edges_in_plan,
        },
        "missing_in_merge": missing_in_merge if FULL_LISTS else missing_in_merge[:200],
        "extra_in_merge": extra_in_merge if FULL_LISTS else extra_in_merge[:200],
        "version_mismatches": version_mismatches if FULL_LISTS else version_mismatches[:200],
        "use_mismatches": use_mismatches if FULL_LISTS else use_mismatches[:200],
        "use_mismatches_common": use_mismatches_common if FULL_LISTS else use_mismatches_common[:200],
        "action_mismatches": action_mismatches if FULL_LISTS else action_mismatches[:200],
        "assumptions": ass,
    }


def find_pairs(root: Path) -> List[Tuple[Path, Path]]:
    pairs: List[Tuple[Path, Path]] = []
    for dirpath, _dirnames, filenames in os.walk(root):
        for fn in filenames:
            if not fn.endswith(".merge"):
                continue
            merge_path = Path(dirpath) / fn
            emerge_path = merge_path.with_suffix(".emerge")
            if emerge_path.exists():
                pairs.append((merge_path, emerge_path))
    return pairs


def main(argv: Sequence[str]) -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", default="/Volumes/Storage/Graph/portage", type=str)
    ap.add_argument("--limit", type=int, default=0, help="Only process first N pairs (0 = all).")
    ap.add_argument("--jobs", type=int, default=max(1, (os.cpu_count() or 2) // 2))
    ap.add_argument("--target-regex", type=str, default="", help="Only process pairs whose basename matches regex.")
    ap.add_argument("--full-lists", action="store_true", help="Include full per-pair missing/extra/mismatch lists in output JSON.")
    ap.add_argument("--md5-cache", type=str, default="", help="Path to md5-cache dir for dependency-aware metrics (DepConc%%, Viol%%).")
    ap.add_argument("--out", type=str, default="/Users/pvdabeel/Desktop/Prolog/Reports/merge_emerge_compare.json")
    args = ap.parse_args(list(argv))
    global FULL_LISTS, MD5_CACHE_DIR
    FULL_LISTS = bool(args.full_lists)
    if args.md5_cache:
        MD5_CACHE_DIR = Path(args.md5_cache)
        if not MD5_CACHE_DIR.exists():
            print(f"warning: md5-cache dir not found: {MD5_CACHE_DIR}", file=sys.stderr)

    root = Path(args.root)
    if not root.exists():
        print(f"error: root not found: {root}", file=sys.stderr)
        return 2

    t0 = time.time()
    pairs = find_pairs(root)
    if args.target_regex:
        rx = re.compile(args.target_regex)
        pairs = [(m, e) for (m, e) in pairs if rx.search(m.stem)]
    pairs.sort(key=lambda me: str(me[0]))
    if args.limit and args.limit > 0:
        pairs = pairs[: args.limit]

    total = len(pairs)
    print(f"pairs={total} jobs={args.jobs} root={root}")

    results: List[Dict] = []
    agg = Counter()
    assumptions_agg = Counter()
    emerge_status = Counter()
    emerge_error_kinds = Counter()
    merge_pkgs_when_emerge_failed = 0
    # Per-package aggregates for CN mismatches (only for emerge_ok pairs).
    missing_cpn = Counter()  # cat/pkg -> count occurrences in missing_in_merge across pairs
    extra_cpn = Counter()    # cat/pkg -> count occurrences in extra_in_merge across pairs

    def work(pair: Tuple[Path, Path]) -> Dict:
        return compare_pair(pair[0], pair[1])

    with cf.ThreadPoolExecutor(max_workers=args.jobs) as ex:
        futs = [ex.submit(work, p) for p in pairs]
        for i, fut in enumerate(cf.as_completed(futs), 1):
            r = fut.result()
            results.append(r)
            for k, v in r["counts"].items():
                agg[k] += v if isinstance(v, float) else int(v)
            for k, v in r.get("assumptions", {}).items():
                assumptions_agg[k] += int(v)
            if r.get("emerge_ok"):
                emerge_status["emerge_ok_pairs"] += 1
                # Track which cat/pkg dominate CN mismatches.
                for key in r.get("missing_in_merge", []) or []:
                    if isinstance(key, str):
                        cat, pnver = split_cat_pnver(key)
                        pn, _ver = split_pn_ver(pnver)
                        if cat and pn:
                            missing_cpn[f"{cat}/{pn}"] += 1
                for key in r.get("extra_in_merge", []) or []:
                    if isinstance(key, str):
                        cat, pnver = split_cat_pnver(key)
                        pn, _ver = split_pn_ver(pnver)
                        if cat and pn:
                            extra_cpn[f"{cat}/{pn}"] += 1
            else:
                emerge_status["emerge_failed_pairs"] += 1
                ek = r.get("emerge_error_kind") or "unknown"
                emerge_error_kinds[ek] += 1
                merge_pkgs_when_emerge_failed += int(r["counts"].get("merge_pkgs", 0))
            if i % 200 == 0 or i == total:
                dt = time.time() - t0
                print(f"  done {i}/{total} ({dt:.1f}s)")

    results.sort(key=lambda r: r["merge"])

    # Ordering aggregate (Kendall tau)
    total_order_inversions = agg["order_inversions"]
    total_order_pairs = agg["order_pairs"]
    order_pct = (100.0 * (total_order_pairs - total_order_inversions) / total_order_pairs) if total_order_pairs else 0.0

    # Spearman rank correlation (weighted average by number of common CNs)
    spearman_weighted = 0.0
    spearman_weight = 0
    for r in results:
        if r.get("emerge_ok"):
            rho = r["counts"].get("spearman_rho", 0.0)
            n_common = r["counts"].get("order_common_cns", 0)
            if n_common >= 2:
                spearman_weighted += rho * n_common
                spearman_weight += n_common
    spearman_agg = spearman_weighted / spearman_weight if spearman_weight else 0.0

    # Dep-pair concordance aggregate
    total_dep_concordant = agg.get("dep_concordant", 0)
    total_dep_pairs = agg.get("dep_pairs", 0)
    dep_conc_pct = (100.0 * total_dep_concordant / total_dep_pairs) if total_dep_pairs else 0.0

    # Violation rate aggregate
    total_violations = agg.get("violations", 0)
    total_dep_edges = agg.get("dep_edges_in_plan", 0)
    viol_pct = (100.0 * total_violations / total_dep_edges) if total_dep_edges else 0.0

    # Top offenders
    top_missing = sorted(results, key=lambda r: r["counts"]["missing_in_merge"], reverse=True)[:20]
    top_use = sorted(results, key=lambda r: r["counts"]["use_mismatches"], reverse=True)[:20]
    top_use_common = sorted(results, key=lambda r: r["counts"]["use_mismatches_common"], reverse=True)[:20]
    top_ass = sorted(results, key=lambda r: r["assumptions"]["domain_assumptions"], reverse=True)[:20]
    top_missing_cpn = missing_cpn.most_common(50)
    top_extra_cpn = extra_cpn.most_common(50)

    out = {
        "root": str(root),
        "pairs": total,
        "aggregate_counts": dict(agg),
        "aggregate_assumptions": dict(assumptions_agg),
        "emerge_status": dict(emerge_status),
        "emerge_error_kinds": dict(emerge_error_kinds),
        "merge_pkgs_when_emerge_failed": merge_pkgs_when_emerge_failed,
        "top_missing": [{"merge": r["merge"], "n": r["counts"]["missing_in_merge"]} for r in top_missing],
        "top_missing_cpn": [{"cpn": cpn, "n": int(n)} for (cpn, n) in top_missing_cpn],
        "top_extra_cpn": [{"cpn": cpn, "n": int(n)} for (cpn, n) in top_extra_cpn],
        "top_use_mismatch": [{"merge": r["merge"], "n": r["counts"]["use_mismatches"]} for r in top_use],
        "top_use_mismatch_common": [{"merge": r["merge"], "n": r["counts"]["use_mismatches_common"]} for r in top_use_common],
        "top_domain_assumptions": [{"merge": r["merge"], "n": r["assumptions"]["domain_assumptions"]} for r in top_ass],
        "order_aggregate": {
            "order_inversions": total_order_inversions,
            "order_pairs": total_order_pairs,
            "order_concordance_pct": round(order_pct, 2),
            "spearman_rho": round(spearman_agg, 4),
            "dep_concordant": total_dep_concordant,
            "dep_discordant": total_dep_pairs - total_dep_concordant,
            "dep_pairs": total_dep_pairs,
            "dep_concordance_pct": round(dep_conc_pct, 2),
            "violations": total_violations,
            "dep_edges_in_plan": total_dep_edges,
            "violation_pct": round(viol_pct, 2),
        },
        "results": results,
    }

    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps(out, indent=2, sort_keys=True))

    dt = time.time() - t0
    print()
    print("SUMMARY")
    print(f"  pairs: {total}")
    print(f"  time_s: {dt:.1f}")
    print(f"  missing_total: {agg['missing_in_merge']}")
    print(f"  extra_total: {agg['extra_in_merge']}")
    print(f"  version_mismatches_total: {agg['version_mismatches']}")
    print(f"  use_mismatches_total: {agg['use_mismatches']}")
    print(f"  use_mismatches_common_total: {agg['use_mismatches_common']}")
    print(f"  use_only_in_emerge_flags_total: {agg['use_only_in_emerge_flags_total']}")
    print(f"  use_only_in_merge_flags_total: {agg['use_only_in_merge_flags_total']}")
    print(f"  use_common_flags_total: {agg['use_common_flags_total']}")
    print(f"  action_mismatches_total: {agg['action_mismatches']}")
    print(f"  emerge_ok_pairs: {emerge_status['emerge_ok_pairs']}")
    print(f"  emerge_failed_pairs: {emerge_status['emerge_failed_pairs']}")
    print(f"  domain_assumptions_total: {assumptions_agg['domain_assumptions']}")
    print(f"  tree_conflict_assumptions_total: {assumptions_agg['tree_conflict_assumptions']}")
    print(f"  blockers_total: {assumptions_agg['blockers']}")
    print(f"  cycle_breaks_total: {assumptions_agg['cycle_breaks']}")
    print(f"  order_inversions: {total_order_inversions}")
    print(f"  order_pairs: {total_order_pairs}")
    print(f"  order_concordance_pct: {order_pct:.2f}%")
    print(f"  spearman_rho: {spearman_agg:.4f}")
    if total_dep_pairs:
        print(f"  dep_concordant: {total_dep_concordant}")
        print(f"  dep_pairs: {total_dep_pairs}")
        print(f"  dep_concordance_pct: {dep_conc_pct:.2f}%")
    if total_dep_edges:
        print(f"  violations: {total_violations}")
        print(f"  dep_edges_in_plan: {total_dep_edges}")
        print(f"  violation_pct: {viol_pct:.2f}%")
    print(f"  install_only_cycle_breaks: {assumptions_agg.get('install_only_cycle_breaks', 0)}")
    print(f"  wrote: {out_path}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))

